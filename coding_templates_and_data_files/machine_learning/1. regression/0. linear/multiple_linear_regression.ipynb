{"cells":[{"cell_type":"markdown","metadata":{},"source":["Multiple Linear Regression"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Importing the libraries\n","import numpy as np\n","import pandas as pd\n","\n","# Importing the dataset\n","dataset = pd.read_csv('50_startups.csv')\n","X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, 4].values\n","\n","\n","# Encoding categorical data\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","ct = ColumnTransformer(\n","    [('one_hot_encoder', OneHotEncoder(categories='auto'), [3])],   # The column numbers to be transformed (here is [0] but can be [0, 1, 3])\n","    remainder='passthrough'                                         # Leave the rest of the columns untouched\n",")\n","X = ct.fit_transform(X)\n","\n","# Avoiding the Dummy Variable Trap\n","X = X[:, 1:] # it will less or delete One dummy variable i.e from 3 to 2 dummy variables\n","\n","\n","# Spliting the dataset into the Training set & Test set\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, \n","                                                    y, \n","                                                    test_size = 0.2, #20%  of whole dataset will be test dataset\n","                                                    random_state = 0)\n","# Fitting Multiple Linear REgression to the Training set\n","from sklearn.linear_model import LinearRegression\n","regressor = LinearRegression()\n","regressor.fit(X_train, y_train)\n","\n","# Predictng the Test set results\n","y_pred = regressor.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["### Building the optimal model using Backward Elimination# Building the optimal model using Backward Elimination\n","Backward Elimination\n","1. Select a significance level to stay in the model (e.g SL = 0.05)\n","2. Fit the full model with all possible predictors\n","3. Consider the predictor with the highest P-values. \n","    - if P>SL, go to STEP 4, otherwise go to Finish\n","4. Remove the predictor\n","5. Fit model without this variable *"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 1. Select a significance level to stay in the model (e.g SL = 0.05)\n","#import statsmodels.formula.api as sm #by course\n","import statsmodels.regression.linear_model as sm\n","# X = np.append(arr = X, \n","#              values = np.ones((50, 1)).astype(int), #ccolumn of 50 rows and 1 column\n","#              axis = 1) # this will append column with values 1 at end of X \n","# to bring at 1st exchange values\n","X = np.append(arr = np.ones((50, 1)).astype(int), \n","              values = X,\n","              axis = 1)\n","#X_opt = X[:, [0,1,2,3,4,5]] #by course\n","X_opt = np.array(X[:, [0, 1, 2, 3, 4, 5]], dtype=float)\n","\n","#--------------------------------------------------------------#\n","\n","# 2. Fit the full model with all possible predictors\n","# OLS stands for OrdinaryLeastSquares\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","\n","#--------------------------------------------------------------#\n","\n","# 3. Consider the predictor with the highest P-values. \n","    #if P>SL, go to STEP 4, otherwise go to Finish\n","print(regressor_OLS.summary())\n","\n","#--------------------------------------------------------------#\n","\n","# 4. Remove the predictor\n","# the 3rd dummy variable which is at index 2 has highest P value and greater than 0.05\n","# Remove index 2 column\n","X_opt = np.array(X[:, [0, 1, 3, 4, 5]], dtype=float)\n","\n","#--------------------------------------------------------------#\n","\n","# 5. Fit model without this variable *\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","\n","#--------------------------------------------------------------#"]},{"cell_type":"markdown","metadata":{},"source":["Rinse & Repeat"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Repeat step 3 and so on\n","print(regressor_OLS.summary())\n","# the 2nd dummy variable which is at index 1 has highest P value and greater than 0.05\n","# Remove index 1 column\n","X_opt = np.array(X[:, [0, 3, 4, 5]], dtype=float)\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","print(regressor_OLS.summary())\n","\n","# the administration variable which is at index 2 has highest P value and greater than 0.05\n","# Remove index 2 column\n","X_opt = np.array(X[:, [0, 3, 5]], dtype=float)\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","print(regressor_OLS.summary())\n","\n","\n","# the marketing variable which is at index 2 has highest P value and greater than 0.05\n","# Remove index 2 column\n","X_opt = np.array(X[:, [0, 3]], dtype=float)\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","print(regressor_OLS.summary())\n","\n","# the R&D is the only powerful predictor to predict the profit in true sense because its P value is very very small and has greater significance"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 ('machine-learning-101')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"485e316128b3797c758008a1fc8409c595c8633b971ed03da2eaa419c7cde595"}}},"nbformat":4,"nbformat_minor":2}
