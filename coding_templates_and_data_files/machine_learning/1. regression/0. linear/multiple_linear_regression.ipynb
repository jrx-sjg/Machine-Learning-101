{"cells":[{"cell_type":"markdown","metadata":{},"source":["Multiple Linear Regression"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Importing the libraries\n","import numpy as np\n","import pandas as pd\n","\n","# Importing the dataset\n","dataset = pd.read_csv('50_startups.csv')\n","X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, 4].values\n","\n","\n","# Encoding categorical data\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","ct = ColumnTransformer(\n","    [('one_hot_encoder', OneHotEncoder(categories='auto'), [3])],   # The column numbers to be transformed (here is [0] but can be [0, 1, 3])\n","    remainder='passthrough'                                         # Leave the rest of the columns untouched\n",")\n","X = ct.fit_transform(X)\n","\n","# Avoiding the Dummy Variable Trap\n","X = X[:, 1:] # it will less or delete One dummy variable i.e from 3 to 2 dummy variables\n","\n","\n","# Spliting the dataset into the Training set & Test set\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, \n","                                                    y, \n","                                                    test_size = 0.2, #20%  of whole dataset will be test dataset\n","                                                    random_state = 0)\n","# Fitting Multiple Linear REgression to the Training set\n","from sklearn.linear_model import LinearRegression\n","regressor = LinearRegression()\n","regressor.fit(X_train, y_train)\n","\n","# Predictng the Test set results\n","y_pred = regressor.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["### Building the optimal model using Backward Elimination# Building the optimal model using Backward Elimination\n","Backward Elimination\n","1. Select a significance level to stay in the model (e.g SL = 0.05)\n","2. Fit the full model with all possible predictors\n","3. Consider the predictor with the highest P-values. \n","    - if P>SL, go to STEP 4, otherwise go to Finish\n","4. Remove the predictor\n","5. Fit model without this variable *"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.951\n","Model:                            OLS   Adj. R-squared:                  0.945\n","Method:                 Least Squares   F-statistic:                     169.9\n","Date:                Thu, 08 Dec 2022   Prob (F-statistic):           1.34e-27\n","Time:                        13:42:25   Log-Likelihood:                -525.38\n","No. Observations:                  50   AIC:                             1063.\n","Df Residuals:                      44   BIC:                             1074.\n","Df Model:                           5                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\n","x1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\n","x2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\n","x3             0.8060      0.046     17.369      0.000       0.712       0.900\n","x4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\n","x5             0.0270      0.017      1.574      0.123      -0.008       0.062\n","==============================================================================\n","Omnibus:                       14.782   Durbin-Watson:                   1.283\n","Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\n","Skew:                          -0.948   Prob(JB):                     2.41e-05\n","Kurtosis:                       5.572   Cond. No.                     1.45e+06\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 1.45e+06. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n"]}],"source":["# 1. Select a significance level to stay in the model (e.g SL = 0.05)\n","#import statsmodels.formula.api as sm #by course\n","import statsmodels.regression.linear_model as sm\n","# X = np.append(arr = X, \n","#              values = np.ones((50, 1)).astype(int), #ccolumn of 50 rows and 1 column\n","#              axis = 1) # this will append column with values 1 at end of X \n","# to bring at 1st exchange values\n","X = np.append(arr = np.ones((50, 1)).astype(int), \n","              values = X,\n","              axis = 1)\n","#X_opt = X[:, [0,1,2,3,4,5]] #by course\n","X_opt = np.array(X[:, [0, 1, 2, 3, 4, 5]], dtype=float)\n","\n","#--------------------------------------------------------------#\n","\n","# 2. Fit the full model with all possible predictors\n","# OLS stands for OrdinaryLeastSquares\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","\n","#--------------------------------------------------------------#\n","\n","# 3. Consider the predictor with the highest P-values. \n","    #if P>SL, go to STEP 4, otherwise go to Finish\n","print(regressor_OLS.summary())\n","\n","#--------------------------------------------------------------#\n","\n","# 4. Remove the predictor\n","# the 3rd dummy variable which is at index 2 has highest P value and greater than 0.05\n","# Remove index 2 column\n","X_opt = np.array(X[:, [0, 1, 3, 4, 5]], dtype=float)\n","\n","#--------------------------------------------------------------#\n","\n","# 5. Fit model without this variable *\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","\n","#--------------------------------------------------------------#"]},{"cell_type":"markdown","metadata":{},"source":["Rinse & Repeat"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.951\n","Model:                            OLS   Adj. R-squared:                  0.946\n","Method:                 Least Squares   F-statistic:                     217.2\n","Date:                Thu, 08 Dec 2022   Prob (F-statistic):           8.49e-29\n","Time:                        13:42:25   Log-Likelihood:                -525.38\n","No. Observations:                  50   AIC:                             1061.\n","Df Residuals:                      45   BIC:                             1070.\n","Df Model:                           4                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const       5.011e+04   6647.870      7.537      0.000    3.67e+04    6.35e+04\n","x1           220.1585   2900.536      0.076      0.940   -5621.821    6062.138\n","x2             0.8060      0.046     17.606      0.000       0.714       0.898\n","x3            -0.0270      0.052     -0.523      0.604      -0.131       0.077\n","x4             0.0270      0.017      1.592      0.118      -0.007       0.061\n","==============================================================================\n","Omnibus:                       14.758   Durbin-Watson:                   1.282\n","Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.172\n","Skew:                          -0.948   Prob(JB):                     2.53e-05\n","Kurtosis:                       5.563   Cond. No.                     1.40e+06\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 1.4e+06. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.951\n","Model:                            OLS   Adj. R-squared:                  0.948\n","Method:                 Least Squares   F-statistic:                     296.0\n","Date:                Thu, 08 Dec 2022   Prob (F-statistic):           4.53e-30\n","Time:                        13:42:25   Log-Likelihood:                -525.39\n","No. Observations:                  50   AIC:                             1059.\n","Df Residuals:                      46   BIC:                             1066.\n","Df Model:                           3                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\n","x1             0.8057      0.045     17.846      0.000       0.715       0.897\n","x2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\n","x3             0.0272      0.016      1.655      0.105      -0.006       0.060\n","==============================================================================\n","Omnibus:                       14.838   Durbin-Watson:                   1.282\n","Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\n","Skew:                          -0.949   Prob(JB):                     2.21e-05\n","Kurtosis:                       5.586   Cond. No.                     1.40e+06\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 1.4e+06. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.950\n","Model:                            OLS   Adj. R-squared:                  0.948\n","Method:                 Least Squares   F-statistic:                     450.8\n","Date:                Thu, 08 Dec 2022   Prob (F-statistic):           2.16e-31\n","Time:                        13:42:25   Log-Likelihood:                -525.54\n","No. Observations:                  50   AIC:                             1057.\n","Df Residuals:                      47   BIC:                             1063.\n","Df Model:                           2                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\n","x1             0.7966      0.041     19.266      0.000       0.713       0.880\n","x2             0.0299      0.016      1.927      0.060      -0.001       0.061\n","==============================================================================\n","Omnibus:                       14.677   Durbin-Watson:                   1.257\n","Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\n","Skew:                          -0.939   Prob(JB):                     2.54e-05\n","Kurtosis:                       5.575   Cond. No.                     5.32e+05\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 5.32e+05. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.947\n","Model:                            OLS   Adj. R-squared:                  0.945\n","Method:                 Least Squares   F-statistic:                     849.8\n","Date:                Thu, 08 Dec 2022   Prob (F-statistic):           3.50e-32\n","Time:                        13:42:25   Log-Likelihood:                -527.44\n","No. Observations:                  50   AIC:                             1059.\n","Df Residuals:                      48   BIC:                             1063.\n","Df Model:                           1                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\n","x1             0.8543      0.029     29.151      0.000       0.795       0.913\n","==============================================================================\n","Omnibus:                       13.727   Durbin-Watson:                   1.116\n","Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\n","Skew:                          -0.911   Prob(JB):                     9.44e-05\n","Kurtosis:                       5.361   Cond. No.                     1.65e+05\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 1.65e+05. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n"]}],"source":["\n","# Repeat step 3 and so on\n","print(regressor_OLS.summary())\n","# the 2nd dummy variable which is at index 1 has highest P value and greater than 0.05\n","# Remove index 1 column\n","X_opt = np.array(X[:, [0, 3, 4, 5]], dtype=float)\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","print(regressor_OLS.summary())\n","\n","# the administration variable which is at index 2 has highest P value and greater than 0.05\n","# Remove index 2 column\n","X_opt = np.array(X[:, [0, 3, 5]], dtype=float)\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","print(regressor_OLS.summary())\n","\n","\n","# the marketing variable which is at index 2 has highest P value and greater than 0.05\n","# Remove index 2 column\n","X_opt = np.array(X[:, [0, 3]], dtype=float)\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","print(regressor_OLS.summary())\n","\n","# the R&D is the only powerful predictor to predict the profit in true sense because its P value is very very small and has greater significance"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 ('machine-learning-101')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"485e316128b3797c758008a1fc8409c595c8633b971ed03da2eaa419c7cde595"}}},"nbformat":4,"nbformat_minor":2}
